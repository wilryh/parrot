% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/scale_text.R
\name{scale_text}
\alias{scale_text}
\title{Scale text using pivoted text scaling}
\usage{
scale_text(tdm, meta = NULL, tdm_vocab = NULL, embeddings = NULL,
  embeddings_vocab = NULL, embeddings_ratio = 1,
  embeddings_count_contribution = 1, compress_fast = FALSE,
  n_dimension_compression = NULL, pivot = 2, verbose = TRUE,
  constrain_outliers = TRUE)
}
\arguments{
\item{tdm}{sparseMatrix. Rows are documents and columns are vocabulary.}

\item{meta}{data.frame. Must line up with tdm etc. This is included only to keep track of any accompanying variables. It is unaltered by the function.}

\item{embeddings}{numeric matrix. A matrix of embedding values.}

\item{embeddings_ratio}{numeric scalar. Ratio of out-of-sample word embeddings to in-sample text for later scaling}

\item{embeddings_count_contribution}{numeric scalar. Fraction of added out-of-sample words to include as pivot words.}

\item{compress_fast}{logical scalar. use R base (F) or RSpectra (T)}

\item{n_dimension_compression}{integer scalar. How many dimensions of PCA to use. The algorithm will not work if this is set too high. If left NULL, a recommended number of dimensions will be calculated automatically.}

\item{pivot}{integer scalar. power of pivot. This should be set as high as possible as long as algorithm still works. 2 or 4 is a good bet. If the method does not converge at 2, try lowering \code{n_dimension_compression} to the sqrt of the vocabulary size. If that does not work, you might need to run without out-of-sample embeddings.}

\item{constrain_outliers}{logical scalar. This requires in-sample words and embedding scores for documents to have approximately unit norms. Recommended for online surveys (reduce influence of bad data), focused survey questions, and online social media data.}
}
\description{
\code{scale_text} runs pivoted text scaling
}
\examples{
\dontrun{
library(stm)
library(parrot)

processed <- textProcessor(
    input_data$text,
    data.frame(input_data),
    removestopwords=T, lowercase=T, stem=F
    )
out <- prepDocuments(
    processed$documents, processed$vocab, processed$meta
    )

tdm <- doc_to_tdm(out)

# download and extract embeddings data first

embeddings <- read_word_embeddings(
    indata=out$vocab,
    ovefile = "path/to/O2M_overlap.txt"
    ## ovefile2 = "path/to/O2M_oov.txt", # very rare words and misspellings
    ## available here http://www.cis.uni-muenchen.de/~wenpeng/renamed-meta-emb.tar.gz
    ## must unpack and replace "path/to/" with location on your computer
    )

scores <- scale_text(
    meta=out$meta,
    tdm=tdm,
    embeddings=as.matrix(
        embeddings[["meta"]
        ),
    compress_fast=TRUE,
    constrain_outliers=TRUE
    )

document_scores <- score_documents(
    scores=scores, n_dimensions=10
    )

get_keywords(scores, n_dimensions=3, n_words=15)

with(document_scores, cor(sqrt(n_words), X0, use="complete"))

plot_keywords(
    scores, x_dimension=1, y_dimension=2, q_cutoff=0.9
    )
}

}
